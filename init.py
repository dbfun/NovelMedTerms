"""
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞.
"""
import os
from pathlib import Path

import nltk
import requests
import spacy.cli
from dotenv import load_dotenv
from gliner import GLiNER
from sqlalchemy import inspect
from transformers import AutoTokenizer

from src.container import container
from src.orm import models
from src.orm.database import BaseModel

_ = models  # –ó–∞—â–∏—Ç–∞ –æ—Ç —É–¥–∞–ª–µ–Ω–∏—è –ª–∏–Ω—Ç–µ—Ä–æ–º.


def drop_tables():
    engine = container.db_engine()
    BaseModel.metadata.drop_all(engine)
    print("‚úÖ –¢–∞–±–ª–∏—Ü—ã —É–¥–∞–ª–µ–Ω—ã")


def create_tables():
    engine = container.db_engine()
    BaseModel.metadata.create_all(engine)
    print("‚úÖ –¢–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã")


def check_tables():
    engine = container.db_engine()

    inspector = inspect(engine)
    tables = inspector.get_table_names()

    expected = set(BaseModel.metadata.tables.keys())
    missing = expected - set(tables)
    if missing:
        print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–±–ª–∏—Ü—ã: {missing}")
    else:
        print("‚úÖ –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã:")
        for table in tables:
            print(f" - {table}")


def load_dictionaries():
    nltk.download("punkt", quiet=True)
    nltk.download("punkt_tab", quiet=True)
    nltk.download("stopwords", quiet=True)
    nltk.download("averaged_perceptron_tagger", quiet=True)
    nltk.download("averaged_perceptron_tagger_eng", quiet=True)


def load_umls_dictionaries():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å UMLS –≤ –≤–∏–¥–µ ZIP-—Ñ–∞–π–ª–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ SQLite3 –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

    –ü–æ —Å—É—Ç–∏ –¥–µ–ª–∞–µ—Ç "Installation", –∫–æ—Ç–æ—Ä—ã–π –æ–ø–∏—Å–∞–Ω –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ pymedtermino2.

    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è pymedtermino2: https://owlready2.readthedocs.io/en/latest/pymedtermino2.html
    """

    def download(zip_path: Path):
        api_key = os.environ["UMLS_API_KEY"]
        url = f"https://uts-ws.nlm.nih.gov/download?url=https://download.nlm.nih.gov/umls/kss/2025AA/umls-2025AA-full.zip&apiKey={api_key}"
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(zip_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=65536):
                    f.write(chunk)

    def convert_to_sqlite3(zip_path: Path, sqlite_path: Path):
        from owlready2.pymedtermino2.umls import default_world, import_umls
        default_world.set_backend(filename=sqlite_path)
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã - –≤–∑—è–ª –∏–∑ —Å–æ—Ä—Ü–æ–≤ owlready2:
        # ['SRC', 'SNOMEDCT_US', 'ICD10', 'ICPC', 'MDR', 'LNC', 'MSH', 'AIR', 'ALT', 'AOD', 'AOT', 'ATC', 'BI', 'CCC', 'CCPSS', 'CCS', 'CCS_10', 'CDT', 'CHV', 'COSTAR', 'CPM', 'CPT', 'CSP', 'CST', 'CVX', 'DDB', 'DRUGBANK', 'DSM-5', 'DXP', 'FMA', 'GO', 'GS', 'HCDT', 'HCPCS', 'HCPT', 'HGNC', 'HL7V2.5', 'HL7V3.0', 'HPO', 'ICD10AE', 'ICD10AM', 'ICD10AMAE', 'ICD10CM', 'ICD10PCS', 'ICD9CM', 'ICF', 'ICF-CY', 'ICNP', 'ICPC2EENG', 'ICPC2ICD10ENG', 'ICPC2P', 'JABL', 'LCH', 'LCH_NW', 'MCM', 'MED-RT', 'MEDCIN', 'MEDLINEPLUS', 'MMSL', 'MMX', 'MTH', 'MTHCMSFRF', 'MTHHH', 'MTHICD9', 'MTHICPC2EAE', 'MTHICPC2ICD10AE', 'MTHMST', 'MTHSPL', 'MVX', 'NANDA-I', 'NCBI', 'NCI', 'NCI_BRIDG', 'NCI_BioC', 'NCI_CDC', 'NCI_CDISC', 'NCI_CDISC-GLOSS', 'NCI_CRCH', 'NCI_CTCAE', 'NCI_CTCAE_3', 'NCI_CTCAE_5', 'NCI_CTEP-SDC', 'NCI_CTRP', 'NCI_CareLex', 'NCI_DCP', 'NCI_DICOM', 'NCI_DTP', 'NCI_FDA', 'NCI_GAIA', 'NCI_GENC', 'NCI_ICH', 'NCI_JAX', 'NCI_KEGG', 'NCI_NCI-GLOSS', 'NCI_NCI-HGNC', 'NCI_NCI-HL7', 'NCI_NCPDP', 'NCI_NICHD', 'NCI_PI-RADS', 'NCI_PID', 'NCI_RENI', 'NCI_UCUM', 'NCI_ZFin', 'NDDF', 'NDFRT', 'NDFRT_FDASPL', 'NDFRT_FMTSME', 'NEU', 'NIC', 'NOC', 'NUCCPT', 'OMIM', 'OMS', 'PCDS', 'PDQ', 'PNDS', 'PPAC', 'PSY', 'QMR', 'RAM', 'RCD', 'RCDAE', 'RCDSA', 'RCDSY', 'RXNORM', 'SNM', 'SNMI', 'SNOMEDCT_VET', 'SOP', 'SPN', 'ULT', 'UMD', 'USP', 'USPMG', 'UWDA', 'VANDF', 'WHO']
        # MSH = MeSH
        import_umls(zip_path,
                    terminologies=["ICD10", "SNOMEDCT_US", "CUI", "WHO", "MSH", "HPO", "GO", "DRUGBANK", "NCI"])
        default_world.save()

    output_dir = Path("resources/dictionaries/umls")
    zip_path = output_dir / "umls-full.zip"
    sqlite_path = output_dir / "pym.sqlite3"

    if zip_path.exists():
        print(f"üì• UMLS —É–∂–µ —Å–∫–∞—á–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é")
    else:
        print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ UMLS ...")
        download(zip_path)
        print(f"‚úÖ –§–∞–π–ª —Å–∫–∞—á–∞–Ω: {zip_path}")

    if sqlite_path.exists():
        print(f"üìÇ UMLS —É–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é")
    else:
        print("üìÇ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ UMLS...")
        convert_to_sqlite3(zip_path, sqlite_path)
        print(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")


def load_hf_models():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å Hugging Face –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç –Ω–µ —Ç—Ä–∞—Ç–∏–ª –≤—Ä–µ–º—è –Ω–∞ —ç—Ç–æ.
    –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ - –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞ (–º–æ–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å —ç—Ç—É —á–∞—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ).
    –î–æ—Å—Ç—É–ø –¥–æ https://huggingface.co –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã.
    –ú–æ–¥–µ–ª–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ ~/.cache/huggingface/hub/
    """
    GLiNER.from_pretrained("Ihor/gliner-biomed-bi-large-v1.0")
    AutoTokenizer.from_pretrained("d4data/biomedical-ner-all")
    print(f"‚úÖ –ú–æ–¥–µ–ª–∏ Hugging Face —Å–∫–∞—á–∞–Ω—ã")


def load_spacy_models():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ spacy
    """
    spacy.cli.download("en_core_web_sm")
    print(f"‚úÖ –ú–æ–¥–µ–ª–∏ spacy —Å–∫–∞—á–∞–Ω—ã")


if __name__ == "__main__":
    load_dotenv()
    assert os.environ["APP_ENV"] == "production"

    drop_tables()
    create_tables()
    check_tables()
    load_dictionaries()
    load_umls_dictionaries()
    load_hf_models()
    load_spacy_models()
